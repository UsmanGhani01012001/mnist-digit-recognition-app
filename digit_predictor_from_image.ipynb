{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17d01f81-8382-46fb-a741-5c4f5fbae140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"mnist_train.csv\")  # Replace with your path\n",
    "\n",
    "# Separate features and labels\n",
    "X = df.drop('label', axis=1).values / 255.0  # Normalize\n",
    "y = to_categorical(df['label'].values, num_classes=10)  # One-hot encode\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55ea7f58-8249-4cbc-95ea-1d5f68f676a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pythonAllcode\\project1\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.5999 - loss: 0.2507 - val_accuracy: 0.9139 - val_loss: 0.0620\n",
      "Epoch 2/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9198 - loss: 0.0548 - val_accuracy: 0.9365 - val_loss: 0.0410\n",
      "Epoch 3/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9460 - loss: 0.0358 - val_accuracy: 0.9491 - val_loss: 0.0327\n",
      "Epoch 4/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9586 - loss: 0.0277 - val_accuracy: 0.9549 - val_loss: 0.0275\n",
      "Epoch 5/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9667 - loss: 0.0223 - val_accuracy: 0.9591 - val_loss: 0.0250\n",
      "Epoch 6/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9748 - loss: 0.0176 - val_accuracy: 0.9649 - val_loss: 0.0219\n",
      "Epoch 7/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9778 - loss: 0.0150 - val_accuracy: 0.9656 - val_loss: 0.0210\n",
      "Epoch 8/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9847 - loss: 0.0118 - val_accuracy: 0.9697 - val_loss: 0.0197\n",
      "Epoch 9/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9866 - loss: 0.0103 - val_accuracy: 0.9701 - val_loss: 0.0191\n",
      "Epoch 10/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.0082 - val_accuracy: 0.9701 - val_loss: 0.0188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b59988b790>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(784,), activation='sigmoid'))  # Hidden layer with Sigmoid\n",
    "model.add(Dense(64, activation='sigmoid'))  # Another hidden layer with Sigmoid\n",
    "model.add(Dense(10, activation='sigmoid'))  # Output layer with Sigmoid (Multi-label style)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1aee280d-87e6-42ed-882d-4b6ba1eb42d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9736 - loss: 0.0181\n",
      "Test Loss: 0.018301019445061684\n",
      "Test Accuracy: 0.9730833172798157\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff290d31-c4fd-4041-a929-722311181876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Accuracy: 0.9730833333333333\n",
      "Precision: 0.9733705946777979\n",
      "Recall: 0.9730833333333333\n",
      "F1 Score: 0.9730863588307693\n",
      "Confusion Matrix:\n",
      " [[1156    0    4    0    3    2    3    2    4    1]\n",
      " [   0 1301   11    3    1    0    1    3    0    2]\n",
      " [   1    3 1153    2    5    0    1    5    3    1]\n",
      " [   0    1   21 1167    1    9    2    7    7    4]\n",
      " [   1    1    1    0 1161    1    3    5    0    3]\n",
      " [   7    1    5   10    5 1058    6    4    6    2]\n",
      " [   3    0    7    0    2    3 1160    0    2    0]\n",
      " [   0    6   12    1    6    0    0 1272    1    1]\n",
      " [   1    3   11    6    5    5    3    2 1122    2]\n",
      " [   3    2    0    5   29    7    0   15    6 1127]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(X_test)                       # Probabilities\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Example: y_true = [actual labels], y_pred = [predicted labels]\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"Precision:\", precision_score(y_true, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_true, y_pred, average='weighted'))\n",
    "print(\"F1 Score:\", f1_score(y_true, y_pred, average='weighted'))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c532264c-68c4-4673-ae9f-5b88c03db844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('mnist_digit_customNN_sigmoid_better.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a722d41-71a7-42f9-b376-04ffe4d2aadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:Matplotlib is building the font cache; this may take a moment.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03ed47b6d6f4db689750f36f2c50019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.jpg,.png,.jpeg', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71bcdfdefe44b1b8703e7cdfab143c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Predict', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6191fd5d5274cfbafd2bc59eb203be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your trained model\n",
    "model = load_model(\"mnist_digit_model.h5\")  # Change to your actual model path\n",
    "\n",
    "# Upload widget\n",
    "upload = widgets.FileUpload(accept='.jpg,.png,.jpeg', multiple=False)\n",
    "display(upload)\n",
    "\n",
    "# Button and output area\n",
    "button = widgets.Button(description=\"Predict\")\n",
    "output = widgets.Output()\n",
    "display(button, output)\n",
    "\n",
    "def preprocess_and_predict(image_data, model):\n",
    "    # Step 1: Load and convert the image to grayscale\n",
    "    image = Image.open(io.BytesIO(image_data)).convert('L')\n",
    "    original_img = np.array(image)\n",
    "\n",
    "    # Resize if the image is too large\n",
    "    if max(original_img.shape) > 300:\n",
    "        original_img = cv2.resize(original_img, (300, 300))\n",
    "\n",
    "    # Step 2: Apply Gaussian blur to reduce noise\n",
    "    blurred = cv2.GaussianBlur(original_img, (5, 5), 0)\n",
    "\n",
    "    # Step 3: Apply adaptive thresholding for better contrast and binarization\n",
    "    thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY_INV, 11, 3)\n",
    "\n",
    "    # Step 4: Find external contours (digits)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        print(\"❌ No digits found.\")\n",
    "        return \"\", original_img, []\n",
    "\n",
    "    # Step 5: Sort contours from left to right based on their x-coordinate\n",
    "    bounding_boxes = [cv2.boundingRect(c) for c in contours]\n",
    "    sorted_boxes = sorted(bounding_boxes, key=lambda b: b[0])\n",
    "\n",
    "    digits = []\n",
    "    predictions = []\n",
    "\n",
    "    for (x, y, w, h) in sorted_boxes:\n",
    "        # Skip too-small contours (likely noise)\n",
    "        if w < 5 or h < 5:\n",
    "            continue\n",
    "\n",
    "        # Step 6: Extract the region of interest (ROI) from the thresholded image\n",
    "        roi = thresh[y:y+h, x:x+w]\n",
    "\n",
    "        # Step 7: Resize the ROI to 20x20, keeping the aspect ratio\n",
    "        if h > w:\n",
    "            new_h = 20\n",
    "            new_w = int(w * (20 / h))\n",
    "        else:\n",
    "            new_w = 20\n",
    "            new_h = int(h * (20 / w))\n",
    "\n",
    "        # Ensure the resize maintains a proper aspect ratio\n",
    "        resized_digit = cv2.resize(roi, (new_w, new_h))\n",
    "\n",
    "        # Step 8: Pad the resized digit to 28x28 (standard MNIST size)\n",
    "        # Use a smaller padding amount to prevent large black borders.\n",
    "        padded = np.pad(resized_digit,\n",
    "                        (((28 - new_h) // 2, (28 - new_h + 1) // 2),\n",
    "                         ((28 - new_w) // 2, (28 - new_w + 1) // 2)),\n",
    "                        mode='constant', constant_values=0)\n",
    "\n",
    "        # Step 9: Normalize the padded image to [0, 1] and flatten it\n",
    "        normalized = padded / 255.0\n",
    "        flattened = normalized.reshape(1, 784)\n",
    "\n",
    "        # Step 10: Make the prediction\n",
    "        pred = model.predict(flattened)\n",
    "        digit = np.argmax(pred)\n",
    "        predictions.append(str(digit))\n",
    "        digits.append(padded)\n",
    "\n",
    "    # Step 11: Combine the individual predictions to form the full number\n",
    "    full_number = ''.join(predictions)\n",
    "    return full_number, original_img, digits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        if upload.value:\n",
    "            file_data = upload.value[0]\n",
    "            image_bytes = file_data['content']\n",
    "\n",
    "            number, original_img, digit_imgs = segment_and_predict(image_bytes, model)\n",
    "\n",
    "            # Show original + segmented digits\n",
    "            fig, axes = plt.subplots(1, len(digit_imgs) + 1, figsize=(10, 4))\n",
    "            axes[0].imshow(original_img, cmap='gray')\n",
    "            axes[0].set_title(\"Original Image\")\n",
    "            axes[0].axis('off')\n",
    "\n",
    "            for i, img in enumerate(digit_imgs):\n",
    "                axes[i + 1].imshow(img, cmap='gray')\n",
    "                axes[i + 1].set_title(f\"Digit {i+1}\")\n",
    "                axes[i + 1].axis('off')\n",
    "\n",
    "            plt.suptitle(f\"✅ Predicted Number: {number}\", fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"❌ Please upload an image first.\")\n",
    "\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89b58df9-d335-495d-91f4-65d833603b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\pythonAllcode\\project1\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7982 - loss: 0.1261 - val_accuracy: 0.9510 - val_loss: 0.0322\n",
      "Epoch 2/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9572 - loss: 0.0288 - val_accuracy: 0.9643 - val_loss: 0.0232\n",
      "Epoch 3/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9726 - loss: 0.0185 - val_accuracy: 0.9688 - val_loss: 0.0208\n",
      "Epoch 4/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9804 - loss: 0.0136 - val_accuracy: 0.9716 - val_loss: 0.0186\n",
      "Epoch 5/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.0098 - val_accuracy: 0.9728 - val_loss: 0.0182\n",
      "Epoch 6/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9899 - loss: 0.0075 - val_accuracy: 0.9751 - val_loss: 0.0166\n",
      "Epoch 7/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9919 - loss: 0.0060 - val_accuracy: 0.9734 - val_loss: 0.0187\n",
      "Epoch 8/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9942 - loss: 0.0048 - val_accuracy: 0.9726 - val_loss: 0.0203\n",
      "Epoch 9/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9965 - loss: 0.0035 - val_accuracy: 0.9746 - val_loss: 0.0192\n",
      "Epoch 10/10\n",
      "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9969 - loss: 0.0031 - val_accuracy: 0.9755 - val_loss: 0.0188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1b5e114c400>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(128, input_shape=(784,), activation='relu'))  # Hidden layer with ReLU\n",
    "model1.add(Dense(64, activation='relu'))  # Another hidden layer with ReLU\n",
    "model1.add(Dense(10, activation='softmax'))  # Output layer with softmax for classification (Multi-class)\n",
    "# Compile the model\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model1.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9931894a-ea41-41f2-8115-1a6a3fa388e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('mnist_digit_customNN_reluSoftmax_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3024703e-777f-46d0-b1e0-1c279d54a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 7.88 GB\n",
      "Used: 5.83 GB\n",
      "Free: 2.05 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total: {mem.total / (1024**3):.2f} GB\")\n",
    "print(f\"Used: {mem.used / (1024**3):.2f} GB\")\n",
    "print(f\"Free: {mem.available / (1024**3):.2f} GB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ChatBoot)",
   "language": "python",
   "name": "project1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
